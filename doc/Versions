Schema Version Control

-------------------------------------------------------------------------------
Note (May 2020)

This document is currently a design specification as full version control has
not yet been implemented in Wyseman.

Objectives:
-------------------------------------------------------------------------------
Wyseman has three fundamental purposes:

- Schema Authoring
  Help the developer code, and make changes to the schema, quickly apply them 
  and evaluate their function in a tight, efficient design iteration cycle.
  This currently works quite well.

- Runtime Access
  Provide applications with an API to the database that is oriented around
  accessing tables, views, functions, reports and the schema data dictionary.
  This is also working well.

- Version Control
  Allow applications to seamlessly update older versions of a schema without
  requiring a full dump/restore and potentially other fiddling by end users.  
  This has not yet been implemented.

-------------------------------------------------------------------------------
Version Control (Feb 2021)

The initial approach of the Ruby/JS port (as of 1.0.17) was to rely on a 
"release" number for each official version of the schema.  This approach has 
several problems:

- It can be difficult to know if a given instantiation of an object in the DB
  belongs to one version or another.  We have the wm.release() function to
  rely on.  But how do we really know for sure about each individual object 
  when trying to update an entire schema from one release to the next?
  
  The initial approach was a precompiled schema file (schema-1.sql) which would
  have all the SQL in it to instantiate a schema of a given version.  But it
  didn't include the "development" parts of the schema such as wm.objects.  The 
  plan was to then create a schema-2.sql, schema-3.sql and so forth as the 
  schema design progressed.
  
  But this would also require migration scripts such as migrate-1-2.sql,
  migrate-2-3.sql and so forth so an app could work with an existing DB 
  instance at a given release level and promote it all the way up to whatever 
  the current version should be.

- Schema components should be capable of being drawn from third-party libraries
  (such as wyselib).  This makes versioning even more complicated.  If my 
  schema version is release 3, but something changes in wyselib, how do I know 
  to promote my application release?  If I always have everything populated in 
  the wm.objects table, I might be able to detect that difference.  But it 
  would be difficult to know simply from a running instance of the database.
  
  The development schema includes a field mod_ver intended somehow for this 
  purpose but hasn't been clear how to use or implement it yet.
  
These issues seem to lead to the conclusion that every object should somehow
deal with its own version control.  If every object could be uniquely 
identified by a hash of its:

  - Create code
  - Dependencies
  - Grants
  - Version number

we could be pretty confident, the current object is the one we want.

Then, the notion of a "release" could consist of a manifest of all the objects 
that belong in that given release, along with their hashes.  This seems to 
simplify a number of issues.

Now a component has no sequential version number, per se.  We just know it 
could get modified.  And we will take steps to make sure the version of it we
are interested in is the one we have in our present release.

Some remaining potential pitfalls:

- How to track an object that was part of a prior release, but has now been
  dropped altogether?  Maybe it is enough to just have it absent from the 
  manifest.  During development, the user would have to wisely use the --prune 
  switch to decide if/when to remove an object that is not part of a current 
  parse.  One would have to be careful if only parsing a subset of all the
  applicable source files.

- How/where to save information about previous versions of database objects.  
  Maybe this is just a variant of the previous point.  The DB meta schema is 
  fully capable of tracking multiple versions of objects. But the flat files 
  are not.  They only represent the current, latest snapshot.  If we delete 
  our database, we will lose all history.  If we want someone else to have
  access to the entire schema history, they would need more than the source
  tree could provide.  There needs to be some file-based archive, part of the 
  schema directory that retains all the old history.

- How to reliaably migrate data from one version of a table to the next when
  the structure of the table may have changed.

-------------------------------------------------------------------------------
Table Data Migration

The main difficulties are presented by:

- New Constraints
  The problem is, the user's table may contain data that worked under the older, 
  more permissive constraints but won't be tolerated under the new constraints.  
  Likely the best we could hope for would be to register a set of queries that 
  would hopefully reveal rows that won't be allowed under the new set of 
  constraints.
  
  A savy developer could use wyseman to run these checks against the production
  database to see what data might need to be massaged prior to running an 
  update script.  A better approach would be a way for apps to do this on their
  own and just report data problems to the user.

- New Columns
  This can easily be handled by proper default values in the new schema.  The
  main problem will be if the developer creates a "not null" constraint but no
  default value.

- Missing Columns
  If a column is renamed or removed rom one version to the next, we really need
  some kind of alter script to run just prior to the drop/create cycle on the
  table.  This will be done in a transaction so it can be safe from other
  accesses going on.  But it will really be up to the developer to write this
  correctly so that data dumped from the old table can be re-imported into the
  new one.
  
  For a renamed column, it is as simple as "alter table rename column x to y."
  
  For a deleted column, there is a chance there is data in that column that may
  need to be presented somehow in some new column.  The developer would have to 
  create the new column, insert data into it based on a query of the old, 
  obsolete column, and then drop the old column.

In short, we could use support for two new features:

- Data Tests
- Version Migrations

Both of these apply only to tables and no other kind of object.

Data tests are limited to queries that will reveal records that have to be 
manually changed before there is any hope of upgrading versions.  We could
probably get by without this feature simply because any attempt at upgrade
would fail if there is offending data in a table.  As long as such attempts are
atomic, there should be no harm in the attempt.

Version Migrations could potentially be any arbitrary SQL that must be executed 
in a transaction immediately before a drop/create cycle on the table.  However
we could simplify things by limiting this to three specific cases where the
columns of a table change:

  - drop w
  - add x
  - rename y to z

From this abstraction we could easily generate SQL to alter the table, just
prior ot the drop/create.  We don't need to affect every possible change to the
table--just enough so the data will dump/restore to the right columns.

The additional benefit is we could conceivably reverse the process if we ever
wanted to revert to a prior version.  Not sure if this is ever needed.  If not,
then the "add x" variant is not really needed.  Initially, let us attempt to 
apply drop and rename migrations and track them in our meta schema.

Our meta schema will need to have provisions for storing and maintaining these
along with the tables they belong to.

-------------------------------------------------------------------------------
History:

It is proposed that Wyseman generate a file in the application schema folder
called Wyseman.history.  This file would hold a structure containing all 
objects a part of prior release commits, but not represented in the set of 
schema files (whether specific to the app, or drawn from any external schema 
libraries).

We will then create a new command line switch to wyseman to accept a table
migration command (drop or rename).  This command would be pushed onto a stack
contained in another new file, Wyseman.migrate, and associated with the table
it belongs to.

Each time wyseman is run, it should consult Wyseman.migrate and load all
migration stacks into the database, noting for new ones that they have not yet
been executed.  Each stack will be stored relational to the applicable table
in wm.objects.

On the next drop/create cycle for a given table, we will first invoke any
commands, in order, that have not yet been executed.  If successful, the table
will have been properly modified, so we can move forward with our work.  If it 
fails for some reason, we will need a way to pop and/or edit the command or 
perhaps push other additional commands before trying again.

When accessed, these objects will get integrated into the history file as well.
The developer will then have to remove the file they were contained in (or make
sure not to parse it again).

Once the schema release is committed, our list of migration commands should be
locked, saved, and become a permanent part of Wyseman.history.  This is likely 
only applicable if the release being committed is >= 2.  Anyone deploying an 
app that uses release 1 should not be presumed to have any pre-existing data.

-------------------------------------------------------------------------------
Bootstrap Schema:

Finally, we should probably resign ourselves to the idea that a production DB
will have to contain wm.objects.  We need a reliable way to know, at least, the
hash for each instantiated object (if not also its drop/create code).

There is probably no compelling reason to have a trimmed-down schema (lacking 
development components) on a production machine.
-------------------------------------------------------------------------------
Data Sources:

There are two basic modes of operation for a WyattERP database.  These need to
be compatible with each other at all times so we can switch back and forth
without anything bad happening.

  - Development:
    - Wyseman CLI is being called directly
    - Load the bootstrap schema automatically if it doesn't exist
    - Load objects from Wyseman.history if they are not already loaded
    - Wait for further calls to wyseman to parse schema description files
    - When that happens:
      - Scan in all schema objects
      - Compare them to what is instantiated in the DB
      - Drop/replace anything that needs to be updated
      - Optionally prune any strays left over
      - Update Wyseman.history file
      - Update Wyseman.migrate file and data
  - Production:
    - Some app is accessing the DB through the Wyseman API
    - Upon launch, check to see if there is an existing database
    - If no existing DB:
      - Populate wm.objects with build data from the schema file
      - Instantiate all database objects to current spec
      - Build data dictionary
      - Initialize all table data
    - Otherwise (there is an existing DB)
      - Check it's version/checksum
      - If the DB is newer than our schema file, abort!
      - If the DB matches our schema file, proceed
      - If it is older than our current schema file:
        - Prepare an SQL transaction to upgrade
        - Try executing it
        - If it succeeds, proceed as normal
        - If it fails, generate logging information about the failure

-------------------------------------------------------------------------------
Strategy:

Instead of a schema file that is straight SQL create code, the following file
format is proposed:

--Schema|<release>|<publish_date>|<schema_hash_of_hashes>
create schema if not exists wm;
create or replace function wm.loader(objs jsonb) as $$
  ... -- loader capable of reading JSON structure below
$$;
wm.loader('{
  "hash":	"Hash of release object hashes",
  "release":	Release_number,
  "published":	"Published Date (null if work-in-process)",
  "bootstrap":	"Encoded bootstrap SQL",
  "initialize":	"Encoded initialization SQL",
  "dictionary":	"Encoded SQL to delete/insert data dictionary tables",
  "objects":	[		#All objects in current release
    "<obj1>":	{"hash": "ABC...", "sql":"Encoded insert data"},
    "<obj2>":	{"hash": "BCD...", "sql":"Encoded insert data"},
    "<obj3>":	{"hash": "CDE...", "sql":"Encoded insert data"},
  ],
  "history": {
    "hash":	"Hash of history hashes",
    "objects": [		#All tables from past releases
      "<obj1>":	{"hash": "ABC...", "sql":"Encoded insert data"},
      "<obj2>":	{"hash": "BCD...", "sql":"Encoded insert data"},
      "<obj3>":	{"hash": "CDE...", "sql":"Encoded insert data"},
  ]},
}');

A schema file of this format should contain enough information to build a new,
empty schema of the current version, or to upgrade an existing database from
any version up to the current.
-------------------------------------------------------------------------------
