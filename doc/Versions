Schema Version Control

Objectives:
-------------------------------------------------------------------------------
Wyseman has three fundamental purposes:

- Schema Authoring
  Help the developer code, and make changes to the schema, quickly apply them 
  and evaluate their function in a tight, efficient design iteration cycle.
  This currently works quite well.

- Runtime Access
  Provide applications with an API to the database that is oriented around
  accessing tables, views, functions, reports and the schema data dictionary.
  This is also working well.

- Version Control
  Allow applications to seamlessly update older versions of a schema without
  requiring a full dump/restore and potentially other fiddling by end users.  
  This has not yet been implemented.

-------------------------------------------------------------------------------
Version Control Enhancements (Dec 2021)

The initial approach of the Ruby/JS port (as of 1.0.17) was to rely on a 
"release" number for each official version of the schema.  This approach has 
several problems:

- It can be difficult to know if a given instantiation of an object in the DB
  belongs to one version or another.  We have the wm.release() function to
  rely on.  But how do we really know for sure about each individual object 
  when trying to update an entire schema from one release to the next?
  
  The initial approach was a precompiled schema file (schema-1.sql) which would
  have all the SQL in it to instantiate a schema of a given version.  But it
  didn't include the "development" parts of the schema such as wm.objects.  The 
  plan was to then create a schema-2.sql, schema-3.sql and so forth as the 
  schema design progressed.
  
  But this would also require migration scripts such as migrate-1-2.sql,
  migrate-2-3.sql and so forth so an app could work with an existing DB 
  instance at a given release level and promote it all the way up to whatever 
  the current version should be.

- Schema components should be capable of being drawn from third-party libraries
  (such as wyselib).  This makes versioning even more complicated.  If my 
  schema version is release 3, but something changes in wyselib, how do I know 
  to promote my application release?  If I always have everything populated in 
  the wm.objects table, I might be able to detect that difference.  But it 
  would be difficult to know simply from a running instance of the database.
  
  The development schema includes a field mod_ver intended somehow for this 
  purpose but hasn't been clear how to use or implement it yet.
  
These issues seem to lead to the conclusion that every object should somehow
deal with its own version control.  If every object could be uniquely 
identified by its:

  - Object type
  - Object name
  - Object version number

Then we could also record a hash of its contents, to also include (at least):
  - Create code
  - Dependencies
  - Grants

Then, the notion of a "release" could consist of a manifest of all the objects 
that belong in that given release, along with their versions and hashes.  This 
seems to simplify a number of issues.

Now a component can record an individual history of its changes, how to
migrate forward into new versions of the schema, and perhaps even how to revert
back to previous versions.

Some remaining potential pitfalls:

- How to track an object that was part of a prior release, but has now been
  dropped altogether?  Maybe it is enough to just have it absent from the 
  manifest.  During development, the user would have to wisely use the --prune 
  switch to decide if/when to remove an object that is not part of a current 
  parse.  One would have to be careful if only parsing a subset of all the
  applicable source files.

- How/where to save information about previous versions of database objects.  
  Maybe this is just a variant of the previous point.  The DB meta schema is 
  fully capable of tracking multiple versions of objects. But the flat files 
  are not.  They only represent the current, latest snapshot.  If we delete 
  our database, we will lose all history.  If we want someone else to have
  access to the entire schema history, they would need more than the source
  tree could provide.  There needs to be some file-based archive, part of the 
  schema directory that retains all the old history.

- How to reliaably migrate data from one version of a table to the next when
  the structure of the table may have changed.

-------------------------------------------------------------------------------
Table Data Migration

The main difficulties are presented by:

- New Constraints
  The problem is, the user's table may contain data that worked under the older, 
  more permissive constraints but won't be tolerated under the new constraints.  
  Likely the best we could hope for would be to register a set of queries that 
  would hopefully reveal rows that won't be allowed under the new set of 
  constraints.
  
  A savy developer could use wyseman to run these checks against the production
  database to see what data might need to be massaged prior to running an 
  update script.  A better approach would be a way for apps to do this on their
  own and just report data problems to the user.

- New Columns
  This can easily be handled by proper default values in the new schema.  The
  main problem will be if the developer creates a "not null" constraint but no
  default value.

- Missing Columns
  If a column is renamed or removed rom one version to the next, we really need
  some kind of alter script to run just prior to the drop/create cycle on the
  table.  This will be done in a transaction so it can be safe from other
  accesses going on.  But it will really be up to the developer to write this
  correctly so that data dumped from the old table can be re-imported into the
  new one.
  
  For a renamed column, it is as simple as "alter table rename column x to y."
  
  For a deleted column, there is a chance there is data in that column that may
  need to be presented somehow in some new column.  The developer would have to 
  create the new column, insert data into it based on a query of the old, 
  obsolete column, and then drop the old column.

In short, we could use support for two new features:

- Data Tests
- Version Migrations

Both of these apply only to tables and no other kind of object.

Data tests are limited to queries that will reveal records that have to be 
manually changed before there is any hope of upgrading versions.  We could
probably get by without this feature simply because any attempt at upgrade
would fail if there is offending data in a table.  As long as such attempts are
atomic, there should be no harm in the attempt.

Version Migrations could potentially be any arbitrary SQL that must be executed 
in a transaction immediately before a drop/create cycle on the table.  It will
simplify things to limit this to a few specific cases where table columns have 
been changed:

  - add column w sql_spec	(initialization expression)
  - drop column x		(revert expression)
  - rename column y z
  - update column expresson	(optional revert expression)

From this abstraction we could likely generate SQL to alter the table, just
prior ot the drop/create.  We don't need to affect every possible change to the
table--just enough so the old data can dump and then restore back into the new
table properly.

Our meta schema will need to have provisions for storing and maintaining these
operations associated with the tables they belong to.  The operations also must
be stored in the order they were created, and are expected to be applied.

-------------------------------------------------------------------------------
History / Migration:

Wyseman will maintain files in the application schema folders called
Wyseman.hist.  This file holds a JSON structure containing the raw SQL 
information (what is in wm.objects) about all objects part of prior releases
but not representing any current changes in the associated schema file.

We will create a new command line switch to wyseman to accept a table
migration operation (delta) such as add, drop, rename, or update).  This
operation will be pushed onto a stack contained in another file called
Wyseman.delta.  The user will also have the ability to simply add delta
operations to that file by hand if they feel comfortable doing so.

Examples of table migration commands:
  - wyseman -g "trees add species 'text' maple"
  - wyseman -g "my.table add address 'text not null' ''"
  - wyseman -g "s.parts drop version 'int not null' 1"
  - wyseman -g "base.contacts rename cell mobile"
  - wyseman -g "items update status '\'closed\' where isnull'"

You can also specify the migration specification as the JSON it will end 
up getting translated to:
  - wyseman -g "{tab:'trees', oper:'add', col:'species', spec:'text' init:'maple'}"
  - wyseman -g "{tab:'contacts' oper:'rename' col:'cell' spec:'mobile'}"

Each time wyseman rebuilds a table, it will check to see if there are new
migration operations in the filesystem that are not yet in the database.  
If so, it will:
  - Check if the table contains no data, complain and abort (as this may not 
    be the case for others who will run the migration).
  - Apply the new migration operations to the table
  - Attempt a dump/restore on the table
  - Append the new operations to the table record in wm.objects (to indicate
    that these delta's have been applied)

In addition to providing operations to the -g switch, you can give the 
following commands (pseudo operations):
  - "table list"	List out the migration commands for this table, 
  			showing what has and has not yet been deployed
  - "table edit"	Edit the migration file for this table
  - "table pop"		Un-apply the last deployed migration and remove
  			it from the stack in the database.  Mark as dirty.
  - "table reset"	Un-apply all deployed migrations and remove them
  			from the stack in the database.  Mark as dirty.

Once a new schema release is committed, our list of migration commands should 
get locked down and become a permanent part of the object, and recorded in
the associated history file.

We should be able to re-create the history file at any time from the locked
object versions contained in the database.  And we should also be able to 
populate an empty database from a valid history (and delta) file.

-------------------------------------------------------------------------------
Bootstrap Schema:

Finally, we will resign to the fact that a production DB will have to  contain 
the wm.objects table.  We need a reliable way to know, at least, the hash for 
each instantiated object.  And it will be much easier to bring and old schema 
current using the build mechanism previously only a part of the development 
environment.

There is probably not that compelling a reason anyway to use a trimmed-down 
schema (lacking development components) in a production database.
-------------------------------------------------------------------------------
Data Sources:

There are two basic modes of operation for a WyattERP database.  These need to
be compatible with each other at all times so we can switch back and forth
without anything bad happening.

  - Development:
    - The schema is under active development/modification
    - The wyseman CLI is being called to deploy changes
    - Load the bootstrap schema automatically if it doesn't exist
    - Load older objects from Wyseman.hist if they are not already loaded
    - Wait for further calls to wyseman to parse schema description files
    - When that happens:
      - Scan in all schema objects
      - Compare them to what is instantiated in the DB
      - Drop/replace anything that needs to be updated
      - Optionally prune any strays left over
      - Update Wyseman.hist file
      - Update Wyseman.delta file and data
    - Allow to commit new release versions

  - Production:
    - Some app is accessing the DB through the Wyseman API
    - Upon launch, check to see if there is an existing database
    - If no existing DB:
      - Populate wm.objects with build data from the schema file
      - Instantiate all database objects to current spec
      - Build data dictionary
      - Initialize all table data
    - Otherwise (there is an existing DB)
      - Check it's version/checksum
      - If the DB is newer than our schema file, abort!
      - If the DB matches our schema file, proceed
      - If the DB is older than our current schema file:
        - Prepare an SQL transaction to upgrade
        - Try executing it
        - If it succeeds, proceed as normal
        - If it fails, abort & generate logging information about the failure

-------------------------------------------------------------------------------
Strategy:

Instead of a schema file that is straight SQL create code, the following JSON
file format is proposed:

{
  current: {
    hash:	"Hash of whole release",
    release:	Release_number,
    publish:	"Published Date (null if work-in-process)",
    boot:	"Encoded bootstrap SQL",
    dict:	"Encoded SQL to delete/insert data dictionary tables",
    init:	"Encoded application schema initialization SQL",
    objects: [		//All objects in current release, in dependency order
      obj1: {
        hash:		"ABC...",
        type:		"View, table, etc",
        name:		"Object name",
        version:	N,
        deps:		[dependencies],
        grants:		[grants],
        column:		[view column data],		//Views only
        create:		"Encoded Sql create code",
        drop:		"Encoded Sql drop code",
        delta:		[{oper 1}, {oper 2}, ...]	//Tables only
        first:		Minimum_release,
        last:		Maximum_release
      },
      obj2:	{Object record as above},
      ...
    ],
  },
  history: [		/All objects from past releases
    obj1:	{Object record as above},
    obj2:	{Object record as above},
    ...
  ]
}

A schema file of this format should contain enough information to build a new, 
empty schema of the current version, or (assuming the history property is 
present) to upgrade an existing database from any version up to the current.

-------------------------------------------------------------------------------
Boot Loader:

A JSON schema file may be wrapped into a self-loading sql chunk as follows:

create schema if not exists wm;
create or replace function wm.loader(objs jsonb) as $$
  ... -- loader capable of reading JSON structure below
$$;
wm.loader('{
  "hash":	"Hash of release object hashes",
  "release":	Release_number,
  ...
}'); drop function wm.loader(jsonb);

This has the advantage that the schema will be built by first populating
wm.objects and then instantiating each of the objects.  This will make future
updating much easier.

-------------------------------------------------------------------------------
